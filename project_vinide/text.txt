In[12]:from sklearn.model_selection  import train_test_splitX, y = mglearn.datasets .make_forge ()X_train, X_test, y_train, y_test = train_test_split (X, y, random_state =0)Next, we import and instantiate the class. This is when we can set parameters, like thenumber of neighbors to use. Here, we set it to 3:In[13]:from sklearn.neighbors  import KNeighborsClassifierclf = KNeighborsClassifier (n_neighbors =3)Now, we fit the classifier using the training set. For KNeighborsClassifier  thismeans storing the dataset, so we can compute neighbors during prediction:In[14]:clf.fit(X_train, y_train)To make predictions on the test data, we call the predict  method. For each data pointin the test set, this computes its nearest neighbors in the training set and finds themost common class among these:In[15]:print("Test set predictions: {}" .format(clf.predict(X_test)))Out[15]:Test set predictions: [1 0 1 0 1 0 0]To evaluate how well our model generalizes, we can call the score  method with thetest data together with the test labels:In[16]:print("Test set accuracy: {:.2f}" .format(clf.score(X_test, y_test)))Out[16]:Test set accuracy: 0.86We see that our model is about 86% accurate, meaning the model predicted the classcorrectly for 86% of the samples in the test dataset.Analyzing 
KNeighborsClassifier
For two-dimensional datasets, we can also illustrate the prediction for all possible testpoints in the xy-plane. We color the plane according to the class that would beassigned to a point in this region. This lets us view the decision boundary , which is thedivide between where the algorithm assigns class 0 versus where it assigns class 1.
Supervised Machine Learning Algorithms | 37The following code produces the visualizations of the decision boundaries for one,three, and nine neighbors shown in Figure 2-6 :In[17]:fig, axes = plt.subplots (1, 3, figsize=(10, 3))for n_neighbors , ax in zip([1, 3, 9], axes):    # the fit method returns the object self, so we can instantiate    # and fit in one line    clf = KNeighborsClassifier (n_neighbors =n_neighbors ).fit(X, y)    mglearn.plots.plot_2d_separator (clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)    mglearn.discrete_scatter (X[:, 0], X[:, 1], y, ax=ax)    ax.set_title ("{} neighbor(s)" .format(n_neighbors ))    ax.set_xlabel ("feature 0" )    ax.set_ylabel ("feature 1" )axes[0].legend(loc=3)
Figure 2-6. Decision boundaries created by the nearest neighbors model for different  val‐ues of n_neighborsAs you can see on the left in the figure, using a single neighbor results in a decisionboundary that follows the training data closely. Considering more and more neigh‐bors leads to a smoother decision boundary. A smoother boundary corresponds to asimpler model. In other words, using few neighbors corresponds to high model com‐plexity (as shown on the right side of Figure 2-1 ), and using many neighbors corre‐sponds to low model complexity (as shown on the left side of Figure 2-1 ). If youconsider the extreme case where the number of neighbors is the number of all datapoints in the training set, each test point would have exactly the same neighbors (alltraining points) and all predictions would be the same: the class that is most frequentin the training set.Let’s investigate whether we can confirm the connection between model complexityand generalization that we discussed earlier. We will do this on the real-world BreastCancer dataset. We begin by splitting the dataset into a training and a test set. Then
38 | Chapter 2: Supervised Learningwe evaluate training and test set performance with different numbers of neighbors.The results are shown in Figure 2-7 :In[18]:from sklearn.datasets  import load_breast_cancercancer = load_breast_cancer ()X_train, X_test, y_train, y_test = train_test_split (    cancer.data, cancer.target, stratify =cancer.target, random_state =66)training_accuracy  = []test_accuracy  = []# try n_neighbors from 1 to 10neighbors_settings  = range(1, 11)for n_neighbors  in neighbors_settings :    # build the model    clf = KNeighborsClassifier (n_neighbors =n_neighbors )    clf.fit(X_train, y_train)    # record training set accuracy    training_accuracy .append(clf.score(X_train, y_train))    # record generalization accuracy    test_accuracy .append(clf.score(X_test, y_test))plt.plot(neighbors_settings , training_accuracy , label="training accuracy" )plt.plot(neighbors_settings , test_accuracy , label="test accuracy" )plt.ylabel("Accuracy" )plt.xlabel("n_neighbors" )plt.legend()The plot shows the training and test set accuracy on the y-axis against the setting ofn_neighbors  on the x-axis. While real-world plots are rarely very smooth, we can stillrecognize some of the characteristics of overfitting and underfitting (note thatbecause considering fewer neighbors corresponds to a more complex model, the plotis horizontally flipped relative to the illustration in Figure 2-1 ). Considering a singlenearest neighbor, the prediction on the training set is perfect. But when more neigh‐bors are considered, the model becomes simpler and the training accuracy drops. Thetest set accuracy for using a single neighbor is lower than when using more neigh‐bors, indicating that using the single nearest neighbor leads to a model that is toocomplex. On the other hand, when considering 10 neighbors, the model is too simpleand performance is even worse. The best performance is somewhere in the middle,using around six neighbors. Still, it is good to keep the scale of the plot in mind. Theworst performance is around 88% accuracy, which might still be acceptable.
Supervised Machine Learning Algorithms | 39Figure 2-7. Comparison of training and test accuracy as a function of n_neighborsk-neighbors regressionThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’sstart by using the single nearest neighbor, this time using the wave  dataset. We’veadded three test data points as green stars on the x-axis. The prediction using a singleneighbor is just the target value of the nearest neighbor. These are shown as blue starsin Figure 2-8 :In[19]:mglearn.plots.plot_knn_regression (n_neighbors =1)
40 | Chapter 2: Supervised LearningFigure 2-8. Predictions made by one-nearest-neighbor regression on the wave datasetAgain, we can use more than the single closest neighbor for regression. When usingmultiple nearest neighbors, the prediction is the average, or mean, of the relevantneighbors ( Figure 2-9 ):In[20]:mglearn.plots.plot_knn_regression (n_neighbors =3)
Supervised Machine Learning Algorithms | 41Figure 2-9. Predictions made by three-nearest-neighbors regression on the wave datasetThe k-nearest neighbors algorithm for regression is implemented in the KNeighborsRegressor  class in scikit-learn . It’s used similarly to KNeighborsClassifier :In[21]:from sklearn.neighbors  import KNeighborsRegressorX, y = mglearn.datasets .make_wave (n_samples =40)# split the wave dataset into a training and a test setX_train, X_test, y_train, y_test = train_test_split (X, y, random_state =0)# instantiate the model and set the number of neighbors to consider to 3reg = KNeighborsRegressor (n_neighbors =3)# fit the model using the training data and training targetsreg.fit(X_train, y_train)Now we can make predictions on the test set:In[22]:print("Test set predictions: \n{}".format(reg.predict(X_test)))
42 | Chapter 2: Supervised LearningOut[22]:Test set predictions:[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]We can also evaluate the model using the score  method, which for regressors returnsthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐ure of goodness of a prediction for a regression model, and yields a score between 0and 1. A value of 1 corresponds to a perfect prediction, and a value of 0 correspondsto a constant model that just predicts the mean of the training set responses, y_train :In[23]:print("Test set R^2: {:.2f}" .format(reg.score(X_test, y_test)))Out[23]:Test set R^2: 0.83Here, the score is 0.83, which indicates a relatively good model fit.Analyzing KNeighborsRegressorFor our one-dimensional dataset, we can see what the predictions look like for allpossible feature values ( Figure 2-10 ). To do this, we create a test dataset consisting ofmany points on the line:In[24]:fig, axes = plt.subplots (1, 3, figsize=(15, 4))# create 1,000 data points, evenly spaced between -3 and 3line = np.linspace (-3, 3, 1000).reshape(-1, 1)for n_neighbors , ax in zip([1, 3, 9], axes):    # make predictions using 1, 3, or 9 neighbors    reg = KNeighborsRegressor (n_neighbors =n_neighbors )    reg.fit(X_train, y_train)    ax.plot(line, reg.predict(line))    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize =8)    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize =8)    ax.set_title (        "{} neighbor(s) \n train score: {:.2f} test score: {:.2f}" .format(            n_neighbors , reg.score(X_train, y_train),            reg.score(X_test, y_test)))    ax.set_xlabel ("Feature" )    ax.set_ylabel ("Target" )axes[0].legend(["Model predictions" , "Training data/target" ,                "Test data/target" ], loc="best")
Supervised Machine Learning Algorithms | 43Figure 2-10. Comparing predictions made by nearest neighbors regression for different
values of n_neighborsAs we can see from the plot, using only a single neighbor, each point in the trainingset has an obvious influence on the predictions, and the predicted values go throughall of the data points. This leads to a very unsteady prediction. Considering moreneighbors leads to smoother predictions, but these do not fit the training data as well.Strengths, weaknesses, and parametersIn principle, there are two important parameters to the KNeighbors  classifier: thenumber of neighbors and how you measure distance between data points. In practice,using a small number of neighbors like three or five often works well, but you shouldcertainly adjust this parameter. Choosing the right distance measure is somewhatbeyond the scope of this book. By default, Euclidean distance is used, which workswell in many settings.One of the strengths of k-NN is that the model is very easy to understand, and oftengives reasonable performance without a lot of adjustments. Using this algorithm is agood baseline method to try before considering more advanced techniques. Buildingthe nearest neighbors model is usually very fast, but when your training set is verylarge (either in number of features or in number of samples) prediction can be slow.When using the k-NN algorithm, it’s important to preprocess your data (see Chap‐ter 3 ). This approach often does not perform well on datasets with many features(hundreds or more), and it does particularly badly with datasets where most featuresare 0 most of the time (so-called sparse datasets ).So, while the nearest k-neighbors algorithm is easy to understand, it is not often usedin practice, due to prediction being slow and its inability to handle many features.The method we discuss next has neither of these drawbacks.
44 | Chapter 2: Supervised LearningLinear ModelsLinear models are a class of models that are widely used in practice and have beenstudied extensively in the last few decades, with roots going back over a hundredyears. Linear models make a prediction using a linear function  of the input features,which we will explain shortly.Linear models for regressionFor regression, the general prediction formula for a linear model looks as follows:ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + bHere, x[0] to x[p] denotes the features (in this example, the number of features is p)of a single data point, w and b are parameters of the model that are learned, and ŷ isthe prediction the model makes. For a dataset with a single feature, this is:ŷ = w[0] * x[0] + bwhich you might remember from high school mathematics as the equation for a line.Here, w[0] is the slope and b is the y-axis offset. For more features, w contains theslopes along each feature axis. Alternatively, you can think of the predicted responseas being a weighted sum of the input features, with weights (which can be negative)given by the entries of w.Trying to learn the parameters w[0] and b on our one-dimensional wave  datasetmight lead to the following line (see Figure 2-11 ):In[25]:mglearn.plots.plot_linear_regression_wave ()Out[25]:w[0]: 0.393906  b: -0.031804
Supervised Machine Learning Algorithms | 45Figure 2-11. Predictions of a linear model on the wave datasetWe added a coordinate cross into the plot to make it easier to understand the line.Looking at w[0]  we see that the slope should be around 0.4, which we can confirmvisually in the plot. The intercept is where the prediction line should cross the y-axis:this is slightly below zero, which you can also confirm in the image.Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature, a plane when using two features, or a hyper‐plane in higher dimensions (that is, when using more features).If you compare the predictions made by the straight line with those made by theKNeighborsRegressor  in Figure 2-10 , using a straight line to make predictions seemsvery restrictive. It looks like all the fine details of the data are lost. In a sense, this istrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear
46 | Chapter 2: Supervised Learning6This is easy to see if you know some linear algebra.combination of the features. But looking at one-dimensional data gives a somewhatskewed perspective. For datasets with many features, linear models can be very pow‐erful. In particular, if you have more features than training data points, any target ycan be perfectly modeled (on the training set) as a linear function.6There are many different linear models for regression. The difference between thesemodels lies in how the model parameters w and b are learned from the training data,and how model complexity can be controlled. We will now take a look at the mostpopular linear models for regression.Linear regression (aka ordinary least squares)Linear regression, or ordinary least squares  (OLS), is the simplest and most classic lin‐ear method for regression. Linear regression finds the parameters w and b that mini‐mize the mean squared error  between predictions and the true regression targets, y,on the training set. The mean squared error is the sum of the squared differencesbetween the predictions and the true values. Linear regression has no parameters,which is a benefit, but it also has no way to control model complexity.Here is the code that produces the model you can see in Figure 2-11 :In[26]:from sklearn.linear_model  import LinearRegressionX, y = mglearn.datasets .make_wave (n_samples =60)X_train, X_test, y_train, y_test = train_test_split (X, y, random_state =42)lr = LinearRegression ().fit(X_train, y_train)The “slope” parameters ( w), also called weights or 
coefficients , are stored in the coef_attribute, while the offset or intercept  (b) is stored in the intercept_  attribute:In[27]:print("lr.coef_: {}" .format(lr.coef_))print("lr.intercept_: {}" .format(lr.intercept_ ))Out[27]:lr.coef_: [ 0.394]lr.intercept_: -0.031804343026759746
Supervised Machine Learning Algorithms | 47Y ou might notice the strange-looking trailing underscore at the endof coef_  and intercept_ . scikit-learn  always stores anythingthat is derived from the training data in attributes that end with atrailing underscore. That is to separate them from parameters thatare set by the user.The intercept_  attribute is always a single float number, while the coef_  attribute isa NumPy array with one entry per input feature. As we only have a single input fea‐ture in the wave  dataset, lr.coef_  only has a single entry.Let’s look at the training set and test set performance:In[28]:print("Training set score: {:.2f}" .format(lr.score(X_train, y_train)))print("Test set score: {:.2f}" .format(lr.score(X_test, y_test)))Out[28]:Training set score: 0.67Test set score: 0.66An R2 of around 0.66 is not very good, but we can see that the scores on the trainingand test sets are very close together. This means we are likely underfitting, not over‐fitting. For this one-dimensional dataset, there is little danger of overfitting, as themodel is very simple (or restricted). However, with higher-dimensional datasets(meaning datasets with a large number of features), linear models become more pow‐erful, and there is a higher chance of overfitting. Let’s take a look at how LinearRegression  performs on a more complex dataset, like the Boston Housing dataset.Remember that this dataset has 506 samples and 105 derived features. First, we loadthe dataset and split it into a training and a test set. Then we build the linear regres‐sion model as before:In[29]:X, y = mglearn.datasets .load_extended_boston ()X_train, X_test, y_train, y_test = train_test_split (X, y, random_state =0)lr = LinearRegression ().fit(X_train, y_train)When comparing training set and test set scores, we find that we predict very accu‐rately on the training set, but the R2 on the test set is much worse:In[30]:print("Training set score: {:.2f}" .format(lr.score(X_train, y_train)))print("Test set score: {:.2f}" .format(lr.score(X_test, y_test)))
48 | Chapter 2: Supervised Learning7Mathematically, Ridge  penalizes the L2 norm of the coefficients, or the Euclidean length of w.Out[30]:Training set score: 0.95Test set score: 0.61This discrepancy between performance on the training set and the test set is a clearsign of overfitting, and therefore we should try to find a model that allows us to con‐trol complexity. One of the most commonly used alternatives to standard linearregression is ridge regression , which we will look into next.Ridge regressionRidge regression is also a linear model for regression, so the formula it uses to makepredictions is the same one used for ordinary least squares. In ridge regression,though, the coefficients ( w) are chosen not only so that they predict well on the train‐ing data, but also to fit an additional constraint. We also want the magnitude of coef‐ficients to be as small as possible; in other words, all entries of w should be close tozero. Intuitively, this means each feature should have as little effect on the outcome aspossible (which translates to having a small slope), while still predicting well. Thisconstraint is an example of what is called regularization . Regularization means explic‐itly restricting a model to avoid overfitting. The particular kind used by ridge regres‐sion is known as L2 regularization.7Ridge regression is implemented in linear_model.Ridge . Let’s see how well it doeson the extended Boston Housing dataset:In[31]:from sklearn.linear_model  import Ridgeridge = Ridge().fit(X_train, y_train)print("Training set score: {:.2f}" .format(ridge.score(X_train, y_train)))print("Test set score: {:.2f}" .format(ridge.score(X_test, y_test)))Out[31]:Training set score: 0.89Test set score: 0.75As you can see, the training set score of Ridge  is lower  than for LinearRegression ,while the test set score is higher . This is consistent with our expectation. With linearregression, we were overfitting our data. Ridge  is a more restricted model, so we areless likely to overfit. A less complex model means worse performance on the trainingset, but better generalization. As we are only interested in generalization perfor‐mance, we should choose the Ridge  model over the LinearRegression  model.
Supervised Machine Learning Algorithms | 49The Ridge  model makes a trade-off between the simplicity of the model (near-zerocoefficients) and its performance on the training set. How much importance themodel places on simplicity versus training set performance can be specified by theuser, using the alpha  parameter. In the previous example, we used the default param‐eter alpha=1.0 . There is no reason why this will give us the best trade-off, though.The optimum setting of alpha  depends on the particular dataset we are using.Increasing alpha  forces coefficients to move more toward zero, which decreasestraining set performance but might help generalization. For example:In[32]:ridge10 = Ridge(alpha=10).fit(X_train, y_train)print("Training set score: {:.2f}" .format(ridge10.score(X_train, y_train)))print("Test set score: {:.2f}" .format(ridge10.score(X_test, y_test)))Out[32]:Training set score: 0.79Test set score: 0.64Decreasing alpha  allows the coefficients to be less restricted, meaning we move rightin Figure 2-1 . For very small values of alpha , coefficients are barely restricted at all,and we end up with a model that resembles LinearRegression :In[33]:ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)print("Training set score: {:.2f}" .format(ridge01.score(X_train, y_train)))print("Test set score: {:.2f}" .format(ridge01.score(X_test, y_test)))Out[33]:Training set score: 0.93Test set score: 0.77Here, alpha=0.1  seems to be working well. We could try decreasing alpha  even moreto improve generalization. For now, notice how the parameter alpha  corresponds tothe model complexity as shown in Figure 2-1 . We will discuss methods to properlyselect parameters in Chapter 5 .We can also get a more qualitative insight into how the alpha  parameter changes themodel by inspecting the coef_  attribute of models with different values of alpha . Ahigher alpha  means a more restricted model, so we expect the entries of coef_  tohave smaller magnitude for a high value of alpha  than for a low value of alpha . Thisis confirmed in the plot in Figure 2-12 :
50 | Chapter 2: Supervised LearningIn[34]:plt.plot(ridge.coef_, 's', label="Ridge alpha=1" )plt.plot(ridge10.coef_, '^', label="Ridge alpha=10" )plt.plot(ridge01.coef_, 'v', label="Ridge alpha=0.1" )plt.plot(lr.coef_, 'o', label="LinearRegression" )plt.xlabel("Coefficient index" )plt.ylabel("Coefficient magnitude" )plt.hlines(0, 0, len(lr.coef_))plt.ylim(-25, 25)plt.legend()
Figure 2-12. Comparing coefficient  magnitudes for ridge regression with different  valuesof alpha and linear regressionHere, the x-axis enumerates the entries of coef_ : x=0 shows the coefficient associatedwith the first feature, x=1 the coefficient associated with the second feature, and so onup to x=100 . The y-axis shows the numeric values of the corresponding values of thecoefficients. The main takeaway here is that for alpha=10 , the coefficients are mostlybetween around –3 and 3. The coefficients for the Ridge  model with alpha=1  aresomewhat larger. The dots corresponding to alpha=0.1  have larger magnitude still,and many of the dots corresponding to linear regression without any regularization(which would be alpha=0 ) are so large they are outside of the chart.
Supervised Machine Learning Algorithms | 51Another way to understand the influence of regularization is to fix a value of alphabut vary the amount of training data available. For Figure 2-13 , we subsampled theBoston Housing dataset and evaluated LinearRegression  and Ridge(alpha=1)  onsubsets of increasing size (plots that show model performance as a function of datasetsize are called learning curves ):In[35]:mglearn.plots.plot_ridge_n_samples ()
Figure 2-13. Learning curves for ridge regression and linear regression on the BostonHousing datasetAs one would expect, the training score is higher than the test score for all datasetsizes, for both ridge and linear regression. Because ridge is regularized, the trainingscore of ridge is lower than the training score for linear regression across the board.However, the test score for ridge is better, particularly for small subsets of the data.For less than 400 data points, linear regression is not able to learn anything. As moreand more data becomes available to the model, both models improve, and linearregression catches up with ridge in the end. The lesson here is that with enough train‐ing data, regularization becomes less important, and given enough data, ridge and
52 | Chapter 2: Supervised Learning8The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values ofthe coefficients.linear regression will have the same performance (the fact that this happens herewhen using the full dataset is just by chance). Another interesting aspect ofFigure 2-13  is the decrease in training performance for linear regression. If more datais added, it becomes harder for a model to overfit, or memorize the data.LassoAn alternative to Ridge  for regularizing linear regression is Lasso . As with ridgeregression, using the lasso also restricts coefficients to be close to zero, but in aslightly different way, called L1 regularization.8 The consequence of L1 regularizationis that when using the lasso, some coefficients are exactly zero . This means some fea‐tures are entirely ignored by the model. This can be seen as a form of automatic fea‐ture selection. Having some coefficients be exactly zero often makes a model easier tointerpret, and can reveal the most important features of your model.Let’s apply the lasso to the extended Boston Housing dataset:In[36]:from sklearn.linear_model  import Lassolasso = Lasso().fit(X_train, y_train)print("Training set score: {:.2f}" .format(lasso.score(X_train, y_train)))print("Test set score: {:.2f}" .format(lasso.score(X_test, y_test)))print("Number of features used: {}" .format(np.sum(lasso.coef_ != 0)))Out[36]:Training set score: 0.29Test set score: 0.21Number of features used: 4As you can see, Lasso  does quite badly, both on the training and the test set. Thisindicates that we are underfitting, and we find that it used only 4 of the 105 features.Similarly to Ridge , the Lasso  also has a regularization parameter, alpha , that controlshow strongly coefficients are pushed toward zero. In the previous example, we usedthe default of alpha=1.0 . To reduce underfitting, let’s try decreasing alpha . When wedo this, we also need to increase the default setting of max_iter  (the maximum num‐ber of iterations to run):
Supervised Machine Learning Algorithms | 53In[37]:# we increase the default setting of "max_iter",# otherwise the model would warn us that we should increase max_iter.lasso001  = Lasso(alpha=0.01, max_iter =100000).fit(X_train, y_train)print("Training set score: {:.2f}" .format(lasso001 .score(X_train, y_train)))print("Test set score: {:.2f}" .format(lasso001 .score(X_test, y_test)))print("Number of features used: {}" .format(np.sum(lasso001 .coef_ != 0)))Out[37]:Training set score: 0.90Test set score: 0.77Number of features used: 33A lower alpha  allowed us to fit a more complex model, which worked better on thetraining and test data. The performance is slightly better than using Ridge , and we areusing only 33 of the 105 features. This makes this model potentially easier to under‐stand.If we set alpha  too low, however, we again remove the effect of regularization and endup overfitting, with a result similar to LinearRegression :In[38]:lasso00001  = Lasso(alpha=0.0001, max_iter =100000).fit(X_train, y_train)print("Training set score: {:.2f}" .format(lasso00001 .score(X_train, y_train)))print("Test set score: {:.2f}" .format(lasso00001 .score(X_test, y_test)))print("Number of features used: {}" .format(np.sum(lasso00001 .coef_ != 0)))Out[38]:Training set score: 0.95Test set score: 0.64Number of features used: 94Again, we can plot the coefficients of the different models, similarly to Figure 2-12 .The result is shown in Figure 2-14 :In[39]:plt.plot(lasso.coef_, 's', label="Lasso alpha=1" )plt.plot(lasso001 .coef_, '^', label="Lasso alpha=0.01" )plt.plot(lasso00001 .coef_, 'v', label="Lasso alpha=0.0001" )plt.plot(ridge01.coef_, 'o', label="Ridge alpha=0.1" )plt.legend(ncol=2, loc=(0, 1.05))plt.ylim(-25, 25)plt.xlabel("Coefficient index" )plt.ylabel("Coefficient magnitude" )
54 | Chapter 2: Supervised LearningFigure 2-14. Comparing coefficient  magnitudes for lasso regression with different  valuesof alpha and ridge regressionFor alpha=1 , we not only see that most of the coefficients are zero (which we alreadyknew), but that the remaining coefficients are also small in magnitude. Decreasingalpha  to 0.01 , we obtain the solution shown as the green dots, which causes mostfeatures to be exactly zero. Using alpha=0.00001 , we get a model that is quite unregu‐larized, with most coefficients nonzero and of large magnitude. For comparison, thebest Ridge  solution is shown in teal. The Ridge  model with alpha=0.1  has similarpredictive performance as the lasso model with alpha=0.01 , but using Ridge , all coef‐ficients are nonzero.In practice, ridge regression is usually the first choice between these two models.However, if you have a large amount of features and expect only a few of them to beimportant, Lasso  might be a better choice. Similarly, if you would like to have amodel that is easy to interpret, Lasso  will provide a model that is easier to under‐stand, as it will select only a subset of the input features. scikit-learn  also providesthe ElasticNet  class, which combines the penalties of Lasso  and Ridge . In practice,this combination works best, though at the price of having two parameters to adjust:one for the L1 regularization, and one for the L2 regularization.
Supervised Machine Learning Algorithms | 55Linear models for classification
Linear models are also extensively used for classification. Let’s look at binary classifi‐cation first. In this case, a prediction is made using the following formula:ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0The formula looks very similar to the one for linear regression, but instead of justreturning the weighted sum of the features, we threshold the predicted value at zero.If the function is smaller than zero, we predict the class –1; if it is larger than zero, wepredict the class +1. This prediction rule is common to all linear models for classifica‐tion. Again, there are many different ways to find the coefficients ( w) and the inter‐cept ( b).For linear models for regression, the output, ŷ, is a linear function of the features: aline, plane, or hyperplane (in higher dimensions). For linear models for classification,the decision boundary  is a linear function of the input. In other words, a (binary) lin‐ear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐plane. We will see examples of that in this section.There are many algorithms for learning linear models. These algorithms all differ inthe following two ways:
•The way in which they measure how well a particular combination of coefficientsand intercept fits the training data
•If and what kind of regularization they useDifferent algorithms choose different ways to measure what “fitting the training setwell” means. For technical mathematical reasons, it is not possible to adjust w and bto minimize the number of misclassifications the algorithms produce, as one mighthope. For our purposes, and many applications, the different choices for item 1 in thepreceding list (called loss functions ) are of little significance.The two most common linear classification algorithms are logistic regression , imple‐mented in linear_model.LogisticRegression , and linear support vector machines(linear SVMs), implemented in svm.LinearSVC  (SVC stands for support vector classi‐fier). Despite its name, LogisticRegression  is a classification algorithm and not aregression algorithm, and it should not be confused with LinearRegression .We can apply the LogisticRegression  and LinearSVC  models to the forge  dataset,and visualize the decision boundary as found by the linear models ( Figure 2-15 ):
56 | Chapter 2: Supervised LearningIn[40]:from sklearn.linear_model  import LogisticRegressionfrom sklearn.svm  import LinearSVCX, y = mglearn.datasets .make_forge ()fig, axes = plt.subplots (1, 2, figsize=(10, 3))for model, ax in zip([LinearSVC (), LogisticRegression ()], axes):    clf = model.fit(X, y)    mglearn.plots.plot_2d_separator (clf, X, fill=False, eps=0.5,                                    ax=ax, alpha=.7)    mglearn.discrete_scatter (X[:, 0], X[:, 1], y, ax=ax)    ax.set_title ("{}".format(clf.__class__ .__name__ ))    ax.set_xlabel ("Feature 0" )    ax.set_ylabel ("Feature 1" )axes[0].legend()
Figure 2-15. Decision boundaries of a linear SVM and logistic regression on the forgedataset with the default parametersIn this figure, we have the first feature of the forge  dataset on the x-axis and the sec‐ond feature on the y-axis, as before. We display the decision boundaries found byLinearSVC  and LogisticRegression  respectively as straight lines, separating the areaclassified as class 1 on the top from the area classified as class 0 on the bottom. Inother words, any new data point that lies above the black line will be classified as class1 by the respective classifier, while any point that lies below the black line will be clas‐sified as class 0.The two models come up with similar decision boundaries. Note that both misclas‐sify two of the points. By default, both models apply an L2 regularization, in the sameway that Ridge  does for regression.For LogisticRegression  and LinearSVC  the trade-off parameter that determines thestrength of the regularization is called C, and higher values of C correspond to less
Supervised Machine Learning Algorithms | 57regularization. In other words, when you use a high value for the parameter C, LogisticRegression  and LinearSVC  try to fit the training set as best as possible, while withlow values of the parameter C, the models put more emphasis on finding a coefficientvector ( w) that is close to zero.There is another interesting aspect of how the parameter C acts. Using low values of Cwill cause the algorithms to try to adjust to the “majority” of data points, while usinga higher value of C stresses the importance that each individual data point be classi‐fied correctly. Here is an illustration using LinearSVC  (Figure 2-16 ):In[41]:mglearn.plots.plot_linear_svc_regularization ()
Figure 2-16. Decision boundaries of a linear SVM on the forge dataset for different
values of COn the lefthand side, we have a very small C corresponding to a lot of regularization.Most of the points in class 0 are at the top, and most of the points in class 1 are at thebottom. The strongly regularized model chooses a relatively horizontal line, misclas‐sifying two points. In the center plot, C is slightly higher, and the model focuses moreon the two misclassified samples, tilting the decision boundary. Finally, on the right‐hand side, the very high value of C in the model tilts the decision boundary a lot, nowcorrectly classifying all points in class 0. One of the points in class 1 is still misclassi‐fied, as it is not possible to correctly classify all points in this dataset using a straightline. The model illustrated on the righthand side tries hard to correctly classify allpoints, but might not capture the overall layout of the classes well. In other words,this model is likely overfitting.Similarly to the case of regression, linear models for classification might seem veryrestrictive in low-dimensional spaces, only allowing for decision boundaries that arestraight lines or planes. Again, in high dimensions, linear models for classification
58 | Chapter 2: Supervised Learningbecome very powerful, and guarding against overfitting becomes increasingly impor‐tant when considering more features.Let’s analyze LinearLogistic  in more detail on the Breast Cancer dataset:In[42]:from sklearn.datasets  import load_breast_cancercancer = load_breast_cancer ()X_train, X_test, y_train, y_test = train_test_split (    cancer.data, cancer.target, stratify =cancer.target, random_state =42)logreg = LogisticRegression ().fit(X_train, y_train)print("Training set score: {:.3f}" .format(logreg.score(X_train, y_train)))print("Test set score: {:.3f}" .format(logreg.score(X_test, y_test)))Out[42]:Training set score: 0.953Test set score: 0.958The default value of C=1 provides quite good performance, with 95% accuracy onboth the training and the test set. But as training and test set performance are veryclose, it is likely that we are underfitting. Let’s try to increase C to fit a more flexiblemodel:In[43]:logreg100  = LogisticRegression (C=100).fit(X_train, y_train)print("Training set score: {:.3f}" .format(logreg100 .score(X_train, y_train)))print("Test set score: {:.3f}" .format(logreg100 .score(X_test, y_test)))Out[43]:Training set score: 0.972Test set score: 0.965Using C=100  results in higher training set accuracy, and also a slightly increased testset accuracy, confirming our intuition that a more complex model should performbetter.We can also investigate what happens if we use an even more regularized model thanthe default of C=1, by setting C=0.01 :In[44]:logreg001  = LogisticRegression (C=0.01).fit(X_train, y_train)print("Training set score: {:.3f}" .format(logreg001 .score(X_train, y_train)))print("Test set score: {:.3f}" .format(logreg001 .score(X_test, y_test)))Out[44]:Training set score: 0.934Test set score: 0.930
Supervised Machine Learning Algorithms | 59As expected, when moving more to the left along the scale shown in Figure 2-1  froman already underfit model, both training and test set accuracy decrease relative to thedefault parameters.Finally, let’s look at the coefficients learned by the models with the three different set‐tings of the regularization parameter C (Figure 2-17 ):In[45]:plt.plot(logreg.coef_.T, 'o', label="C=1")plt.plot(logreg100 .coef_.T, '^', label="C=100")plt.plot(logreg001 .coef_.T, 'v', label="C=0.001" )plt.xticks(range(cancer.data.shape[1]), cancer.feature_names , rotation =90)plt.hlines(0, 0, cancer.data.shape[1])plt.ylim(-5, 5)plt.xlabel("Coefficient index" )plt.ylabel("Coefficient magnitude" )plt.legend()As LogisticRegression  applies an L2 regularization by default,the result looks similar to that produced by Ridge  in Figure 2-12 .Stronger regularization pushes coefficients more and more towardzero, though coefficients never become exactly zero. Inspecting theplot more closely, we can also see an interesting effect in the thirdcoefficient, for “mean perimeter. ” For C=100  and C=1, the coefficientis negative, while for C=0.001 , the coefficient is positive, with amagnitude that is even larger than for C=1. Interpreting a modellike this, one might think the coefficient tells us which class a fea‐ture is associated with. For example, one might think that a high“texture error” feature is related to a sample being “malignant. ”However, the change of sign in the coefficient for “mean perimeter”means that depending on which model we look at, a high “meanperimeter” could be taken as being either indicative of “benign” orindicative of “malignant. ” This illustrates that interpretations ofcoefficients of linear models should always be taken with a grain ofsalt.
60 | Chapter 2: Supervised LearningFigure 2-17. Coefficients  learned by logistic regression on the Breast Cancer dataset for
different  values of C
Supervised Machine Learning Algorithms | 61If we desire a more interpretable model, using L1 regularization might help, as it lim‐its the model to using only a few features. Here is the coefficient plot and classifica‐tion accuracies for L1 regularization ( Figure 2-18 ):In[46]:for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):    lr_l1 = LogisticRegression (C=C, penalty="l1").fit(X_train, y_train)    print("Training accuracy of l1 logreg with C={:.3f}: {:.2f}" .format(          C, lr_l1.score(X_train, y_train)))    print("Test accuracy of l1 logreg with C={:.3f}: {:.2f}" .format(          C, lr_l1.score(X_test, y_test)))    plt.plot(lr_l1.coef_.T, marker, label="C={:.3f}" .format(C))plt.xticks(range(cancer.data.shape[1]), cancer.feature_names , rotation =90)plt.hlines(0, 0, cancer.data.shape[1])plt.xlabel("Coefficient index" )plt.ylabel("Coefficient magnitude" )plt.ylim(-5, 5)plt.legend(loc=3)Out[46]:Training accuracy of l1 logreg with C=0.001: 0.91Test accuracy of l1 logreg with C=0.001: 0.92Training accuracy of l1 logreg with C=1.000: 0.96Test accuracy of l1 logreg with C=1.000: 0.96Training accuracy of l1 logreg with C=100.000: 0.99Test accuracy of l1 logreg with C=100.000: 0.98As you can see, there are many parallels between linear models for binary classifica‐tion and linear models for regression. As in regression, the main difference betweenthe models is the penalty  parameter, which influences the regularization andwhether the model will use all available features or select only a subset.
62 | Chapter 2: Supervised LearningFigure 2-18. Coefficients  learned by logistic regression with L1 penalty on the BreastCancer dataset for 
different  values of CLinear models for multiclass 
classification
Many linear classification models are for binary classification only, and don’t extendnaturally to the multiclass case (with the exception of logistic regression). A commontechnique to extend a binary classification algorithm to a multiclass classificationalgorithm is the one-vs.-rest  approach. In the one-vs.-rest approach, a binary model islearned for each class that tries to separate that class from all of the other classes,resulting in as many binary models as there are classes. To make a prediction, allbinary classifiers are run on a test point. The classifier that has the highest score on itssingle class “wins, ” and this class label is returned as the prediction.
Supervised Machine Learning Algorithms | 63Having one binary classifier per class results in having one vector of coefficients ( w)and one intercept ( b) for each class. The class for which the result of the classificationconfidence formula given here is highest is the assigned class label:w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + bThe mathematics behind multiclass logistic regression differ somewhat from the one-vs.-rest approach, but they also result in one coefficient vector and intercept per class,and the same method of making a prediction is applied.Let’s apply the one-vs.-rest method to a simple three-class classification dataset. Weuse a two-dimensional dataset, where each class is given by data sampled from aGaussian distribution (see Figure 2-19 ):In[47]:from sklearn.datasets  import make_blobsX, y = make_blobs (random_state =42)mglearn.discrete_scatter (X[:, 0], X[:, 1], y)plt.xlabel("Feature 0" )plt.ylabel("Feature 1" )plt.legend(["Class 0" , "Class 1" , "Class 2" ])
Figure 2-19. Two-dimensional toy dataset containing three classes
64 | Chapter 2: Supervised LearningNow, we train a LinearSVC  classifier on the dataset:In[48]:linear_svm  = LinearSVC ().fit(X, y)print("Coefficient shape: " , linear_svm .coef_.shape)print("Intercept shape: " , linear_svm .intercept_ .shape)Out[48]:Coefficient shape:  (3, 2)Intercept shape:  (3,)We see that the shape of the coef_  is (3, 2) , meaning that each row of coef_  con‐tains the coefficient vector for one of the three classes and each column holds thecoefficient value for a specific feature (there are two in this dataset). The intercept_is now a one-dimensional array, storing the intercepts for each class.Let’s visualize the lines given by the three binary classifiers ( Figure 2-20 ):In[49]:mglearn.discrete_scatter (X[:, 0], X[:, 1], y)line = np.linspace (-15, 15)for coef, intercept , color in zip(linear_svm .coef_, linear_svm .intercept_ ,                                  ['b', 'r', 'g']):    plt.plot(line, -(line * coef[0] + intercept ) / coef[1], c=color)plt.ylim(-10, 15)plt.xlim(-10, 8)plt.xlabel("Feature 0" )plt.ylabel("Feature 1" )plt.legend(['Class 0' , 'Class 1' , 'Class 2' , 'Line class 0' , 'Line class 1' ,            'Line class 2' ], loc=(1.01, 0.3))Y ou can see that all the points belonging to class 0 in the training data are above theline corresponding to class 0, which means they are on the “class 0” side of this binaryclassifier. The points in class 0 are above the line corresponding to class 2, whichmeans they are classified as “rest” by the binary classifier for class 2. The pointsbelonging to class 0 are to the left of the line corresponding to class 1, which meansthe binary classifier for class 1 also classifies them as “rest. ” Therefore, any point inthis area will be classified as class 0 by the final classifier (the result of the classifica‐tion confidence formula for classifier 0 is greater than zero, while it is smaller thanzero for the other two classes).But what about the triangle in the middle of the plot? All three binary classifiers clas‐sify points there as “rest. ” Which class would a point there be assigned to? The answeris the one with the highest value for the classification formula: the class of the closestline.
Supervised Machine Learning Algorithms | 65Figure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers
The following example ( Figure 2-21 ) shows the predictions for all regions of the 2Dspace:In[50]:mglearn.plots.plot_2d_classification (linear_svm , X, fill=True, alpha=.7)mglearn.discrete_scatter (X[:, 0], X[:, 1], y)line = np.linspace (-15, 15)for coef, intercept , color in zip(linear_svm .coef_, linear_svm .intercept_ ,                                  ['b', 'r', 'g']):    plt.plot(line, -(line * coef[0] + intercept ) / coef[1], c=color)plt.legend(['Class 0' , 'Class 1' , 'Class 2' , 'Line class 0' , 'Line class 1' ,            'Line class 2' ], loc=(1.01, 0.3))plt.xlabel("Feature 0" )plt.ylabel("Feature 1" )
66 | Chapter 2: Supervised LearningFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers
Strengths, weaknesses, and parametersThe main parameter of linear models is the regularization parameter, called alpha  inthe regression models and C in LinearSVC  and LogisticRegression . Large values foralpha  or small values for C mean simple models. In particular for the regression mod‐els, tuning these parameters is quite important. Usually C and alpha  are searched foron a logarithmic scale. The other decision you have to make is whether you want touse L1 regularization or L2 regularization. If you assume that only a few of your fea‐tures are actually important, you should use L1. Otherwise, you should default to L2.L1 can also be useful if interpretability of the model is important. As L1 will use onlya few features, it is easier to explain which features are important to the model, andwhat the effects of these features are.Linear models are very fast to train, and also fast to predict. They scale to very largedatasets and work well with sparse data. If your data consists of hundreds of thou‐sands or millions of samples, you might want to investigate using the solver='sag'option in LogisticRegression  and Ridge , which can be faster than the default onlarge datasets. Other options are the SGDClassifier  class and the SGDRegressorclass, which implement even more scalable versions of the linear models describedhere.Another strength of linear models is that they make it relatively easy to understandhow a prediction is made, using the formulas we saw earlier for regression and classi‐fication. Unfortunately, it is often not entirely clear why coefficients are the way theyare. This is particularly true if your dataset has highly correlated features; in thesecases, the coefficients might be hard to interpret.
Supervised Machine Learning Algorithms | 67Linear models often perform well when the number of features is large compared tothe number of samples. They are also often used on very large datasets, simplybecause it’s not feasible to train other models. However, in lower-dimensional spaces,other models might yield better generalization performance. We will look at someexamples in which linear models fail in “Kernelized Support Vector Machines” onpage 92 .Method ChainingThe fit method of all scikit-learn  models returns self . This allows you to writecode like the following, which we’ve already used extensively in this chapter:In[51]:# instantiate model and fit it in one linelogreg = LogisticRegression ().fit(X_train, y_train)Here, we used the return value of fit (which is self ) to assign the trained model tothe variable logreg . This concatenation of method calls (here __init__  and then fit)is known as method chaining . Another common application of method chaining inscikit-learn  is to fit and predict  in one line:In[52]:logreg = LogisticRegression ()y_pred = logreg.fit(X_train, y_train).predict(X_test)Finally, you can even do model instantiation, fitting, and predicting in one line:In[53]:y_pred = LogisticRegression ().fit(X_train, y_train).predict(X_test)This very short variant is not ideal, though. A lot is happening in a single line, whichmight make the code hard to read. Additionally, the fitted logistic regression modelisn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.Naive Bayes 
Classifiers
Naive Bayes classifiers are a family of classifiers that are quite similar to the linearmodels discussed in the previous section. However, they tend to be even faster intraining. The price paid for this efficiency is that naive Bayes models often providegeneralization performance that is slightly worse than that of linear classifiers likeLogisticRegression  and LinearSVC .The reason that naive Bayes models are so efficient is that they learn parameters bylooking at each feature individually and collect simple per-class statistics from eachfeature. There are three kinds of naive Bayes classifiers implemented in scikit-
68 | Chapter 2: Supervised Learninglearn : GaussianNB , BernoulliNB , and MultinomialNB . GaussianNB  can be applied toany continuous data, while BernoulliNB  assumes binary data and MultinomialNBassumes count data (that is, that each feature represents an integer count of some‐thing, like how often a word appears in a sentence). BernoulliNB  and MultinomialNBare mostly used in text data classification.The BernoulliNB  classifier counts how often every feature of each class is not zero.This is most easily understood with an example:In[54]:X = np.array([[0, 1, 0, 1],              [1, 0, 1, 1],              [0, 0, 0, 1],              [1, 0, 1, 0]])y = np.array([0, 1, 0, 1])Here, we have four data points, with four binary features each. There are two classes,0 and 1. For class 0 (the first and third data points), the first feature is zero two timesand nonzero zero times, the second feature is zero one time and nonzero one time,and so on. These same counts are then calculated for the data points in the secondclass. Counting the nonzero entries per class in essence looks like this:In[55]:counts = {}for label in np.unique(y):    # iterate over each class    # count (sum) entries of 1 per feature    counts[label] = X[y == label].sum(axis=0)print("Feature counts: \n{}".format(counts))Out[55]:Feature counts:{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}The other two naive Bayes models, MultinomialNB  and GaussianNB , are slightly dif‐ferent in what kinds of statistics they compute. MultinomialNB  takes into account theaverage value of each feature for each class, while GaussianNB  stores the average valueas well as the standard deviation of each feature for each class.To make a prediction, a data point is compared to the statistics for each of the classes,and the best matching class is predicted. Interestingly, for both MultinomialNB  andBernoulliNB , this leads to a prediction formula that is of the same form as in the lin‐ear models (see “Linear models for classification” on page 56). Unfortunately, coef_for the naive Bayes models has a somewhat different meaning than in the linear mod‐els, in that coef_  is not the same as w.
Supervised Machine Learning Algorithms | 69Strengths, weaknesses, and parametersMultinomialNB  and BernoulliNB  have a single parameter, alpha , which controlsmodel complexity. The way alpha  works is that the algorithm adds to the data alphamany virtual data points that have positive values for all the features. This results in a“smoothing” of the statistics. A large alpha  means more smoothing, resulting in lesscomplex models. The algorithm’s performance is relatively robust to the setting ofalpha , meaning that setting alpha  is not critical for good performance. However,tuning it usually improves accuracy somewhat.GaussianNB  is mostly used on very high-dimensional data, while the other two var‐iants of naive Bayes are widely used for sparse count data such as text. MultinomialNBusually performs better than BinaryNB , particularly on datasets with a relatively largenumber of nonzero features (i.e., large documents).The naive Bayes models share many of the strengths and weaknesses of the linearmodels. They are very fast to train and to predict, and the training procedure is easyto understand. The models work very well with high-dimensional sparse data and arerelatively robust to the parameters. Naive Bayes models are great baseline models andare often used on very large datasets, where training even a linear model might taketoo long.Decision TreesDecision trees are widely used models for classification and regression tasks. Essen‐tially, they learn a hierarchy of if/else questions, leading to a decision.These questions are similar to the questions you might ask in a game of 20 Questions.Imagine you want to distinguish between the following four animals: bears, hawks,penguins, and dolphins. Y our goal is to get to the right answer by asking as few if/elsequestions as possible. Y ou might start off by asking whether the animal has feathers, aquestion that narrows down your possible animals to just two. If the answer is “yes, ”you can ask another question that could help you distinguish between hawks andpenguins. For example, you could ask whether the animal can fly. If the animaldoesn’t have feathers, your possible animal choices are dolphins and bears, and youwill need to ask a question to distinguish between these two animals—for example,asking whether the animal has fins.This series of questions can be expressed as a decision tree, as shown in Figure 2-22 .In[56]:mglearn.plots.plot_animal_tree ()
70 | Chapter 2: Supervised LearningFigure 2-22. A decision tree to distinguish among several animalsIn this illustration, each node in the tree either represents a question or a terminalnode (also called a leaf) that contains the answer. The edges connect the answers to aquestion with the next question you would ask.In machine learning parlance, we built a model to distinguish between four classes ofanimals (hawks, penguins, dolphins, and bears) using the three features “has feath‐ers, ” “can fly, ” and “has fins. ” Instead of building these models by hand, we can learnthem from data using supervised learning.Building decision treesLet’s go through the process of building a decision tree for the 2D classification data‐set shown in Figure 2-23 . The dataset consists of two half-moon shapes, with eachclass consisting of 75 data points. We will refer to this dataset as two_moons .Learning a decision tree means learning the sequence of if/else questions that gets usto the true answer most quickly. In the machine learning setting, these questions arecalled tests (not to be confused with the test set, which is the data we use to test to seehow generalizable our model is). Usually data does not come in the form of binaryyes/no features as in the animal example, but is instead represented as continuousfeatures such as in the 2D dataset shown in Figure 2-23 . The tests that are used oncontinuous data are of the form “Is feature i larger than value a?”
Supervised Machine Learning Algorithms | 71Figure 2-23. Two-moons dataset on which the decision tree will be builtTo build a tree, the algorithm searches over all possible tests and finds the one that ismost informative about the target variable. Figure 2-24  shows the first test that ispicked. Splitting the dataset vertically at x[1]=0.0596  yields the most information; itbest separates the points in class 1 from the points in class 2. The top node, also calledthe root, represents the whole dataset, consisting of 75 points belonging to class 0 and75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596 ,indicated by a black line. If the test is true, a point is assigned to the left node, whichcontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwisethe point is assigned to the right node, which contains 48 points belonging to class 0and 18 points belonging to class 1. These two nodes correspond to the top and bot‐tom regions shown in Figure 2-24 . Even though the first split did a good job of sepa‐rating the two classes, the bottom region still contains points belonging to class 0, andthe top region still contains points belonging to class 1. We can build a more accuratemodel by repeating the process of looking for the best test in both regions.Figure 2-25  shows that the most informative next split for the left and the right regionis based on x[0] .
72 | Chapter 2: Supervised LearningFigure 2-24. Decision boundary of tree with depth 1 (left)  and corresponding tree (right)
Figure 2-25. Decision boundary of tree with depth 2 (left)  and corresponding decisiontree (right)This recursive process yields a binary tree of decisions, with each node containing atest. Alternatively, you can think of each test as splitting the part of the data that iscurrently being considered along one axis. This yields a view of the algorithm asbuilding a hierarchical partition. As each test concerns only a single feature, theregions in the resulting partition always have axis-parallel boundaries.The recursive partitioning of the data is repeated until each region in the partition(each leaf in the decision tree) only contains a single target value (a single class or asingle regression value). A leaf of the tree that contains data points that all share thesame target value is called pure . The final partitioning for this dataset is shown inFigure 2-26 .
Supervised Machine Learning Algorithms | 73Figure 2-26. Decision boundary of tree with depth 9 (left)  and part of the correspondingtree (right); the full tree is quite large and hard to visualizeA prediction on a new data point is made by checking which region of the partitionof the feature space the point lies in, and then predicting the majority target (or thesingle target in the case of pure leaves) in that region. The region can be found bytraversing the tree from the root and going left or right, depending on whether thetest is fulfilled or not.It is also possible to use trees for regression tasks, using exactly the same technique.To make a prediction, we traverse the tree based on the tests in each node and findthe leaf the new data point falls into. The output for this data point is the mean targetof the training points in this leaf.Controlling complexity of decision treesTypically, building a tree as described here and continuing until all leaves are pureleads to models that are very complex and highly overfit to the training data. Thepresence of pure leaves mean that a tree is 100% accurate on the training set; eachdata point in the training set is in a leaf that has the correct majority class. The over‐fitting can be seen on the left of Figure 2-26 . Y ou can see the regions determined tobelong to class 1 in the middle of all the points belonging to class 0. On the otherhand, there is a small strip predicted as class 0 around the point belonging to class 0to the very right. This is not how one would imagine the decision boundary to look,and the decision boundary focuses a lot on single outlier points that are far awayfrom the other points in that class.There are two common strategies to prevent overfitting: stopping the creation of thetree early (also called pre-pruning ), or building the tree but then removing or collaps‐ing nodes that contain little information (also called post-pruning  or just pruning ).Possible criteria for pre-pruning include limiting the maximum depth of the tree,limiting the maximum number of leaves, or requiring a minimum number of pointsin a node to keep splitting it.
74 | Chapter 2: Supervised LearningDecision trees in scikit-learn  are implemented in the DecisionTreeRegressor  andDecisionTreeClassifier  classes. scikit-learn  only implements pre-pruning, notpost-pruning.Let’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. Asalways, we import the dataset and split it into a training and a test part. Then we builda model using the default setting of fully developing the tree (growing the tree untilall leaves are pure). We fix the random_state  in the tree, which is used for tie-breaking internally:In[58]:from sklearn.tree  import DecisionTreeClassifiercancer = load_breast_cancer ()X_train, X_test, y_train, y_test = train_test_split (    cancer.data, cancer.target, stratify =cancer.target, random_state =42)tree = DecisionTreeClassifier (random_state =0)tree.fit(X_train, y_train)print("Accuracy on training set: {:.3f}" .format(tree.score(X_train, y_train)))print("Accuracy on test set: {:.3f}" .format(tree.score(X_test, y_test)))Out[58]:Accuracy on training set: 1.000Accuracy on test set: 0.937As expected, the accuracy on the training set is 100%—because the leaves are pure,the tree was grown deep enough that it could perfectly memorize all the labels on thetraining data. The test set accuracy is slightly worse than for the linear models welooked at previously, which had around 95% accuracy.If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deepand complex. Unpruned trees are therefore prone to overfitting and not generalizingwell to new data. Now let’s apply pre-pruning to the tree, which will stop developingthe tree before we perfectly fit to the training data. One option is to stop building thetree after a certain depth has been reached. Here we set max_depth=4 , meaning onlyfour consecutive questions can be asked (cf. Figures 2-24  and 2-26 ). Limiting thedepth of the tree decreases overfitting. This leads to a lower accuracy on the trainingset, but an improvement on the test set:In[59]:tree = DecisionTreeClassifier (max_depth =4, random_state =0)tree.fit(X_train, y_train)print("Accuracy on training set: {:.3f}" .format(tree.score(X_train, y_train)))print("Accuracy on test set: {:.3f}" .format(tree.score(X_test, y_test)))
Supervised Machine Learning Algorithms | 75Out[59]:Accuracy on training set: 0.988Accuracy on test set: 0.951Analyzing decision treesWe can visualize the tree using the export_graphviz  function from the tree  module.This writes a file in the .dot file format, which is a text file format for storing graphs.We set an option to color the nodes to reflect the majority class in each node and passthe class and features names so the tree can be properly labeled:In[61]:from sklearn.tree  import export_graphvizexport_graphviz (tree, out_file ="tree.dot" , class_names =["malignant" , "benign" ],                feature_names =cancer.feature_names , impurity =False, filled=True)We can read this file and visualize it, as seen in Figure 2-27 , using the graphviz  mod‐ule (or you can use any program that can read .dot files):In[61]:import graphvizwith open("tree.dot" ) as f:    dot_graph  = f.read()graphviz .Source(dot_graph )
Figure 2-27. Visualization of the decision tree built on the Breast Cancer dataset
76 | Chapter 2: Supervised LearningThe visualization of the tree provides a great in-depth view of how the algorithmmakes predictions, and is a good example of a machine learning algorithm that iseasily explained to nonexperts. However, even with a tree of depth four, as seen here,the tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐mon) are even harder to grasp. One method of inspecting the tree that may be helpfulis to find out which path most of the data actually takes. The n_samples  shown ineach node in Figure 2-27  gives the number of samples in that node, while value  pro‐vides the number of samples per class. Following the branches to the right, we seethat worst radius <= 16.795  creates a node that contains only 8 benign but 134malignant samples. The rest of this side of the tree then uses some finer distinctionsto split off these 8 remaining benign samples. Of the 142 samples that went to theright in the initial split, nearly all of them (132) end up in the leaf to the very right.Taking a left at the root, for worst radius > 16.795  we end up with 25 malignantand 259 benign samples. Nearly all of the benign samples end up in the second leaffrom the right, with most of the other leaves containing very few samples.Feature importance in treesInstead of looking at the whole tree, which can be taxing, there are some useful prop‐erties that we can derive to summarize the workings of the tree. The most commonlyused summary is feature importance , which rates how important each feature is forthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0means “not used at all” and 1 means “perfectly predicts the target. ” The featureimportances always sum to 1:In[62]:print("Feature importances: \n{}".format(tree.feature_importances_ ))Out[62]:Feature importances:[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]We can visualize the feature importances in a way that is similar to the way we visual‐ize the coefficients in the linear model ( Figure 2-28 ):In[63]:def plot_feature_importances_cancer (model):    n_features  = cancer.data.shape[1]    plt.barh(range(n_features ), model.feature_importances_ , align='center' )    plt.yticks(np.arange(n_features ), cancer.feature_names )    plt.xlabel("Feature importance" )    plt.ylabel("Feature" )plot_feature_importances_cancer (tree)
Supervised Machine Learning Algorithms | 77Figure 2-28. Feature importances computed from a decision tree learned on the BreastCancer datasetHere we see that the feature used in the top split (“worst radius”) is by far the mostimportant feature. This confirms our observation in analyzing the tree that the firstlevel already separates the two classes fairly well.However, if a feature has a low feature_importance , it doesn’t mean that this featureis uninformative. It only means that the feature was not picked by the tree, likelybecause another feature encodes the same information.In contrast to the coefficients in linear models, feature importances are always posi‐tive, and don’t encode which class a feature is indicative of. The feature importancestell us that “worst radius” is important, but not whether a high radius is indicative of asample being benign or malignant. In fact, there might not be such a simple relation‐ship between features and class, as you can see in the following example (Figures 2-29and 2-30 ):In[64]:tree = mglearn.plots.plot_tree_not_monotone ()display(tree)Out[64]:Feature importances: [ 0.  1.]
78 | Chapter 2: Supervised LearningFigure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐notonous relationship with the class label, and the decision boundaries found by a deci‐sion tree
Figure 2-30. Decision tree learned on the data shown in Figure 2-29The plot shows a dataset with two features and two classes. Here, all the informationis contained in X[1] , and X[0]  is not used at all. But the relation between X[1]  and
Supervised Machine Learning Algorithms | 79the output class is not monotonous, meaning we cannot say “a high value of X[0]means class 0, and a low value means class 1” (or vice versa).While we focused our discussion here on decision trees for classification, all that wassaid is similarly true for decision trees for regression, as implemented in DecisionTreeRegressor . The usage and analysis of regression trees is very similar to that ofclassification trees. There is one particular property of using tree-based models forregression that we want to point out, though. The DecisionTreeRegressor  (and allother tree-based regression models) is not able to extrapolate , or make predictionsoutside of the range of the training data.Let’s look into this in more detail, using a dataset of historical computer memory(RAM) prices. Figure 2-31  shows the dataset, with the date on the x-axis and the priceof one megabyte of RAM in that year on the y-axis:In[65]:import pandas as pdram_prices  = pd.read_csv ("data/ram_price.csv" )plt.semilogy (ram_prices .date, ram_prices .price)plt.xlabel("Year")plt.ylabel("Price in $/Mbyte" )
Figure 2-31. Historical development of the price of RAM, plotted on a log scale
80 | Chapter 2: Supervised LearningNote the logarithmic scale of the y-axis. When plotting logarithmically, the relationseems to be quite linear and so should be relatively easy to predict, apart from somebumps.We will make a forecast for the years after 2000 using the historical data up to thatpoint, with the date as our only feature. We will compare two simple models: aDecisionTreeRegressor  and LinearRegression . We rescale the prices using a loga‐rithm, so that the relationship is relatively linear. This doesn’t make a difference forthe DecisionTreeRegressor , but it makes a big difference for LinearRegression  (wewill discuss this in more depth in Chapter 4 ). After training the models and makingpredictions, we apply the exponential map to undo the logarithm transform. Wemake predictions on the whole dataset for visualization purposes here, but for aquantitative evaluation we would only consider the test dataset:In[66]:from sklearn.tree  import DecisionTreeRegressor# use historical data to forecast prices after the year 2000data_train  = ram_prices [ram_prices .date < 2000]data_test  = ram_prices [ram_prices .date >= 2000]# predict prices based on dateX_train = data_train .date[:, np.newaxis]# we use a log-transform to get a simpler relationship of data to targety_train = np.log(data_train .price)tree = DecisionTreeRegressor ().fit(X_train, y_train)linear_reg  = LinearRegression ().fit(X_train, y_train)# predict on all dataX_all = ram_prices .date[:, np.newaxis]pred_tree  = tree.predict(X_all)pred_lr = linear_reg .predict(X_all)# undo log-transformprice_tree  = np.exp(pred_tree )price_lr  = np.exp(pred_lr)Figure 2-32 , created here, compares the predictions of the decision tree and the linearregression model with the ground truth:In[67]:plt.semilogy (data_train .date, data_train .price, label="Training data" )plt.semilogy (data_test .date, data_test .price, label="Test data" )plt.semilogy (ram_prices .date, price_tree , label="Tree prediction" )plt.semilogy (ram_prices .date, price_lr , label="Linear prediction" )plt.legend()
Supervised Machine Learning Algorithms | 819It is actually possible to make very good forecasts with tree-based models (for example, when trying to predictwhether a price will go up or down). The point of this example was not to show that trees are a bad model fortime series, but to illustrate a particular property of how trees make predictions.
Figure 2-32. Comparison of predictions made by a linear model and predictions madeby a regression tree on the RAM price dataThe difference between the models is quite striking. The linear model approximatesthe data with a line, as we knew it would. This line provides quite a good forecast forthe test data (the years after 2000), while glossing over some of the finer variations inboth the training and the test data. The tree model, on the other hand, makes perfectpredictions on the training data; we did not restrict the complexity of the tree, so itlearned the whole dataset by heart. However, once we leave the data range for whichthe model has data, the model simply keeps predicting the last known point. The treehas no ability to generate “new” responses, outside of what was seen in the trainingdata. This shortcoming applies to all models based on trees.9Strengths, weaknesses, and parametersAs discussed earlier, the parameters that control model complexity in decision treesare the pre-pruning parameters that stop the building of the tree before it is fullydeveloped. Usually, picking one of the pre-pruning strategies—setting either
82 | Chapter 2: Supervised Learningmax_depth , max_leaf_nodes , or min_samples_leaf —is sufficient to prevent overfit‐ting.Decision trees have two advantages over many of the algorithms we’ve discussed sofar: the resulting model can easily be visualized and understood by nonexperts (atleast for smaller trees), and the algorithms are completely invariant to scaling of thedata. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of featuresis needed for decision tree algorithms. In particular, decision trees work well whenyou have features that are on completely different scales, or a mix of binary and con‐tinuous features.The main downside of decision trees is that even with the use of pre-pruning, theytend to overfit and provide poor generalization performance. Therefore, in mostapplications, the ensemble methods we discuss next are usually used in place of a sin‐gle decision tree.Ensembles of Decision TreesEnsembles  are methods that combine multiple machine learning models to createmore powerful models. There are many models in the machine learning literaturethat belong to this category, but there are two ensemble models that have proven tobe effective on a wide range of datasets for classification and regression, both ofwhich use decision trees as their building blocks: random forests and gradient boos‐ted decision trees.Random forestsAs we just observed, a main drawback of decision trees is that they tend to overfit thetraining data. Random forests are one way to address this problem. A random forestis essentially a collection of decision trees, where each tree is slightly different fromthe others. The idea behind random forests is that each tree might do a relativelygood job of predicting, but will likely overfit on part of the data. If we build manytrees, all of which work well and overfit in different ways, we can reduce the amountof overfitting by averaging their results. This reduction in overfitting, while retainingthe predictive power of the trees, can be shown using rigorous mathematics.To implement this strategy, we need to build many decision trees. Each tree should doan acceptable job of predicting the target, and should also be different from the othertrees. Random forests get their name from injecting randomness into the tree build‐ing to ensure each tree is different. There are two ways in which the trees in a randomforest are randomized: by selecting the data points used to build a tree and by select‐ing the features in each split test. Let’s go into this process in more detail.
Supervised Machine Learning Algorithms | 83Building random forests.    To build a random forest model, you need to decide on thenumber of trees to build (the n_estimators  parameter of RandomForestRegressor  orRandomForestClassifier ). Let’s say we want to build 10 trees. These trees will bebuilt completely independently from each other, and the algorithm will make differ‐ent random choices for each tree to make sure the trees are distinct. To build a tree,we first take what is called a bootstrap sample  of our data. That is, from our n_samplesdata points, we repeatedly draw an example randomly with replacement (meaning thesame sample can be picked multiple times), n_samples  times. This will create a data‐set that is as big as the original dataset, but some data points will be missing from it(approximately one third), and some will be repeated.To illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b','c', 'd'] . A possible bootstrap sample would be ['b', 'd', 'd', 'c'] . Anotherpossible sample would be ['d', 'a', 'd', 'a'] .Next, a decision tree is built based on this newly created dataset. However, the algo‐rithm we described for the decision tree is slightly modified. Instead of looking forthe best test for each node, in each node the algorithm randomly selects a subset ofthe features, and it looks for the best possible test involving one of these features. Thenumber of features that are selected is controlled by the max_features  parameter.This selection of a subset of features is repeated separately in each node, so that eachnode in a tree can make a decision using a different subset of the features.The bootstrap sampling leads to each decision tree in the random forest being builton a slightly different dataset. Because of the selection of features in each node, eachsplit in each tree operates on a different subset of features. Together, these two mech‐anisms ensure that all the trees in the random forest are different.A critical parameter in this process is max_features . If we set max_features  to n_features , that means that each split can look at all features in the dataset, and no ran‐domness will be injected in the feature selection (the randomness due to thebootstrapping remains, though). If we set max_features  to 1, that means that thesplits have no choice at all on which feature to test, and can only search over differentthresholds for the feature that was selected randomly. Therefore, a high max_features  means that the trees in the random forest will be quite similar, and they will beable to fit the data easily, using the most distinctive features. A low max_featuresmeans that the trees in the random forest will be quite different, and that each treemight need to be very deep in order to fit the data well.To make a prediction using the random forest, the algorithm first makes a predictionfor every tree in the forest. For regression, we can average these results to get our finalprediction. For classification, a “soft voting” strategy is used. This means each algo‐rithm makes a “soft” prediction, providing a probability for each possible output
84 | Chapter 2: Supervised Learninglabel. The probabilities predicted by all the trees are averaged, and the class with thehighest probability is predicted.
Analyzing random forests.    Let’s apply a random forest consisting of five trees to thetwo_moons  dataset we studied earlier:In[68]:from sklearn.ensemble  import RandomForestClassifierfrom sklearn.datasets  import make_moonsX, y = make_moons (n_samples =100, noise=0.25, random_state =3)X_train, X_test, y_train, y_test = train_test_split (X, y, stratify =y,                                                    random_state =42)forest = RandomForestClassifier (n_estimators =5, random_state =2)forest.fit(X_train, y_train)The trees that are built as part of the random forest are stored in the estimator_attribute. Let’s visualize the decision boundaries learned by each tree, together withtheir aggregate prediction as made by the forest ( Figure 2-33 ):In[69]:fig, axes = plt.subplots (2, 3, figsize=(20, 10))for i, (ax, tree) in enumerate (zip(axes.ravel(), forest.estimators_ )):    ax.set_title ("Tree {}" .format(i))    mglearn.plots.plot_tree_partition (X_train, y_train, tree, ax=ax)mglearn.plots.plot_2d_separator (forest, X_train, fill=True, ax=axes[-1, -1],                                alpha=.4)axes[-1, -1].set_title ("Random Forest" )mglearn.discrete_scatter (X_train[:, 0], X_train[:, 1], y_train)Y ou can clearly see that the decision boundaries learned by the five trees are quite dif‐ferent. Each of them makes some mistakes, as some of the training points that areplotted here were not actually included in the training sets of the trees, due to thebootstrap sampling.The random forest overfits less than any of the trees individually, and provides amuch more intuitive decision boundary. In any real application, we would use manymore trees (often hundreds or thousands), leading to even smoother boundaries.
Supervised Machine Learning Algorithms | 85Figure 2-33. Decision boundaries found by five randomized decision trees and the deci‐sion boundary obtained by averaging their predicted probabilitiesAs another example, let’s apply a random forest consisting of 100 trees on the BreastCancer dataset:In[70]:X_train, X_test, y_train, y_test = train_test_split (    cancer.data, cancer.target, random_state =0)forest = RandomForestClassifier (n_estimators =100, random_state =0)forest.fit(X_train, y_train)print("Accuracy on training set: {:.3f}" .format(forest.score(X_train, y_train)))print("Accuracy on test set: {:.3f}" .format(forest.score(X_test, y_test)))Out[70]:Accuracy on training set: 1.000Accuracy on test set: 0.972The random forest gives us an accuracy of 97%, better than the linear models or asingle decision tree, without tuning any parameters. We could adjust the max_features  setting, or apply pre-pruning as we did for the single decision tree. However,often the default parameters of the random forest already work quite well.Similarly to the decision tree, the random forest provides feature importances, whichare computed by aggregating the feature importances over the trees in the forest. Typ‐ically, the feature importances provided by the random forest are more reliable thanthe ones provided by a single tree. Take a look at Figure 2-34 .
86 | Chapter 2: Supervised Learning